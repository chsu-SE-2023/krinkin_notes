Время работы алгоритма в первую очередь зависит не от скорости работы компьютера, а от сути самого алгоритма. При оценке сложности используют понятие "длина последовательности входных данных". Длина может измеряться в битах, символах, количестве чисел и др. В любом случае требуется найти зависимость между длиной входной последовательности $(n)$ и скоростью работы алгоритма.

Если алгоритм состоит из одного цикла, однократно перебирающего все n значений говорят что это `линейный` алгоритм и его сложность пропорциональна n. Причём на практике не принципиально будет ли алгоритм перебирать все $n$, $n\over 2$, $n\over k$ в этом случае время работы алгоритма будет пропорционально $1\over{k*n}$, но при увеличении значения $n$ влияние коэффициента будет несущественным.

С практической точки зрения важно знать верхнюю границу хуже которой алгоритм работать не будет. В теории алгоритмов её обозначают с помощью $O$-символики. Говорят, что сложность алгоритма $O(f(n))$ если начиная с некоторого `n[i]` сложность ограничена сверху функцией $C*f(n)$.

Если внутри алгоритма `два вложенных цикла`, каждый из которых перебирает n элементов сложность составит $O(f(n^2))$.

Количество перестановок - $O(f(!n))$

Если у нас имеется две части алгоритма, работающие со сложностью $n^2$, то общая сложность останется $n^2$.

%%Как будто бы n математически можно считать бесконечностью%%