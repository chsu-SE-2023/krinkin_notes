**Большие данные (Big Data)** представляют собой новое качество обычных данных в электронном виде, накопленных в большом объеме в разнообразных информационных системах, корпоративных или государственных сайтах, блогах  
V-модель Больших данных:
- Variety - разнообразие данных, работа с разными источниками
- Volume - объем
- Velocity - скорость поступления или обработки
- Validity - надежность данных, возможность ошибок на каждом этапе
- Value - ценность данных (релевантность, определяется задачей)
- Veracity - один из вариантов надежности, доверие к способам обработки может быть разным, не интерпретируемые методы к нему приводят 
#### Как развить интеллект компьютера?

**Нейробиологический подход** - предполагает искусственное воспроизведение искусственным способом мыслительного процесса человека, создание технического аналога биологической системы. -> первые нейронные сети

**Информационный подход** - основан на создании специальных средств решения задач, которые относят к интеллектуальным, на компьютере. -> развитие математических методов, алгоритмов, языков программирования.

#### Использование Больших данных и машинного обучения

Есть миллион медицинских снимков. Что с ними можно сделать?  
Нанять миллион человек? У нас не хватит денег. Мы их не найдем, если это узкие специалисты. Нужно много времени чтобы все посмотреть и сделать выводы.  
Вывод: лучше бы это делать не вручную  
Здесь и возникает машинное обучение - алгоритмы, которые будут просматривать эти снимки и делать выводы

#### Машинное обучение (Machine Learning) и интеллектуальный анализ данных (Data Mining)

Сильно пересекаются (методы в целом одни и те же)  
**Data Mining** - поиск закономерностей в данных  
**Машинное обучение** - эксплуатация этих закономерностей  
Выделяем признаки, которые характеризуют снимки из примера выше. Data Mining интерпретирует признаки, а Machine Learning использует их для автоматического распознавания диагноза  
**Машинное обучение (Machine Learning)** - обширный подраздел ИИ, изучающий методы построения алгоритмов, способных обучаться  
**Искусственный интеллект (Artifical Intelligence, AI)** - раздел информатики, изучающий возможность обеспечения разумных рассуждений и действий с помощью вычислительных систем и иных искусственных устройств  
**Сильный ИИ** осознает себя, способен самостоятельно мыслить и может заменить человека практически во всех сферах  
**Слабый ИИ** реализует какие-либо аспекты, похожие на мышление. Например, компьютерное зрение, машинный перевод, голосовые помощники  

#### Обучение искусственного интеллекта
##### Обучение без учителя
**Обучение без учителя (самообучение, спонтанное обучение, Unsupervised learning)** - один из способов машинного обучения, при котором испытуемая система спонтанно обучается выполнять поставленную задачу без вмешательства со стороны экспериментатора. 
Мы не знаем, что можем получить, у нас нет никаких образцов, которые могут быть использованы, чтобы из этих данных что-то получить, т. е. это поиск закономерностей всех видов.
Методы:
- Методы кластеризации
- Методы поиска закономерностей
- Частотные методы
Для этих методов нет конкретно описанного алгоритма, только для метода кластеризации существуют некоторые конкретные алгоритмы, для остальных методов есть алгоритмы только для решения конкретных специфических задач.
##### Обучение с учителем
**Обучение с учителем (Supervised learning)** - один из способов машинного обучения, в ходе которого испытуемая система принудительно обучается на обучающей выборке. На основе этих данных требуется восстановить зависимость, т. е. построить алгоритм, эффективно работающий на новых данных (пригодный для прогнозирования).
У нас есть образцы, по этим образцам мы настраиваем некоторую модель, и эта модель принимает некоторые решения после обучения, для того, чтобы облегчить нам жизнь на новых поступивших данных.
Методы:
- Предиктивные методы
- Методы прогнозирования
- Методы классификации

Эти методы часто используются вместе - мы можем найти закономерности, используя методы без учителя, затем поставить эти методы в обучение с учителем и эту связку эксплуатировать для более-менее автономной работы.

Существуют также **методы обучения с подкреплением** - не алгоритмизированные методы, в которых делается предположение о том, как будет вести себя модель, после чего на основе правильных данных модель обучается, а при добавлении данных и их ошибочной обработки - модель корректируется.

#### Искусственные нейронные сети

**Искусственные нейронные сети** - математические модели, а также их программные или аппаратные реализации, построенные по принципу организации и функционирования биологических нейронных сетей - сетей нервных клеток живого организма.
В мозгу человека порядка 80-90 млрд нейронов. В искусственных нейросетях - десятки тысяч нейронов.

#### Этапы развития искусственных нейронных сетей

- В 50-е годы XX века впервые созданы именно как алгоритм
- В 70-е годы разработаны многие современные алгоритмы
- В конце 90-х годов - сети глубокого обучения  
  
С *математической* точки зрения, обучение нейросетей - это многопараметрическая задача оптимизации.
С точки зрения *машинного обучения*, нейросеть представляет собой частный случай методов распознавания образов, дискриминантного анализа, методов кластеризации и т. д.  
На *технологическом* уровне нейросети используются как черный ящик, который позволяет распознавать образы, используется для тематического перевода и т. д.

#### Предпосылки к использованию интеллектуального анализа данных

- Данные имеют неограниченный объем
- Данные являются являются разнородными (количественными, качественными, текстовыми)
- Инструменты для обработки данных должны быть просты в использовании
- Результаты должны быть конкретны и понятны  
  
Примеры:
- Астрофизика, астрономия
- Множество видео
- Социальные сети
#### Data Mining

**Data Mining** - это процесс обнаружения в сырых данных ранее неизвестных, нетривиальных, практически полезных и доступных интерпретаций знаний, необходимых для принятия решений в различных сферах человеческой деятельности  
Потенциально у нас есть данные, они называются в данном контексте сырыми, они содержат какие-то закономерности, возможно тривиальные.  
Но когда мы их извлекали, мы тратили время, деньги, свое, чужое. Это тоже особенность - когда не знаешь, что получишь  
Задача часто формируется в процессе работы. Это процесс постоянный, интерактивный, со множеством ошибок и нюансов.  
Чтобы этот процесс ввести в определенные рамки, используются стандарты.  
Стандарты есть и для того, что мы называем машинное обучение.  
ИИ, машинное обучение, data mining называют новой отраслью, новой специальностью. Эту отрасль называют **data science - наука о данных**

#### Стандарты Data Science

Три стандарта сферы Data Science:
- KDD
- SEMMA
- CRISP_DM  

Эти стандарты показывают, какие именно этапы нужно выполнить, чтобы получить определенный результат. К каждому этапу можно предъявлять разные требования. Алгоритмы на каждом этап тоже свои. В целом они не накладывают ограничений, но позволяют структурировать их и для каждого этапа либо применить классический набор методов, либо понимать, что мы с каждого этапа получим.
Этапы процесса анализа данных по стандарту CRISP-DM:
1. Понимание бизнеса
2. Понимание данных
3. Подготовка данных
4. Моделирование
5. Оценка
6. Развертывание
7. Обратно в п. 1 (на основании новых данных мы можем переобучать модель)  

![](Excalidraw/01_01.%20Этапы%20процесса%20анализа%20данных%20по%20стандарту%20CRISP-DM.png)  

#### Типы задач анализа данных

1. Поиск частых шаблонов и ассоциативных правил
2. Предикт (предсказание, прогнозирование)
3. Классификация
4. Кластеризация
5. Визуализация
6. Обратно в п. 1  

![](Excalidraw/01_02.%20Типы%20задач%20анализа%20данных.png)
